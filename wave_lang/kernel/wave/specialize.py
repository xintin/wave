# Copyright 2025 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

"""
Specialization in wave.

note. tensor store is not supported yet.

Analysis:
- identify 2 partitions
    - load partition
    - compute partition
    [- store partition]

Workers:
- n_service_waves * waves_per_block[0] waves for load
- waves_per_block[0] * waves_per_block[1] * waves_per_block[2] total waves for compute

Synchronization:
- wave_{i} has named barrier {i+1, i+1+total_waves}
- barId i+1 represents data is ready in buffer
- barId i+1+total_waves represent data is consumed from buffer

- e.g., Assume {wave_0, i=0, total_waves=4}
    - barId=1 is signal_ready:
        - load partition signals compute partition [data is ready]
    - barId=1 is wait_ready:
        - compute partition waiting on load to signal ready.

    - barId=5 is signal_empty:
        - compute partition signals load parition [data is consumed]
    - barId=5 is wait_empty:
        - load partition waiting on compute to signal empty.
"""

import math
from typing import Optional, List
from collections import defaultdict

import torch.fx as fx
from torch.utils import _pytree as pytree

import wave_lang.kernel.lang as tkl

from .._support.tracing import CapturedTrace
from .._support.location import CapturedLocation
from .._support.indexing import IndexSequence, IndexSymbol, IndexExpr

from ..compiler.kernel_codegen import filter_fx_graph
from ..lang.global_symbols import *
from ..ops.wave_ops import (
    MMA,
    Conditional,
    CustomOp,
    Output,
    Ge,
    GetResult,
    Iterate,
    Lt,
    NewScalar,
    Read,
    SharedMemoryBarrierSignal,
    SharedMemoryBarrierWait,
    TensorLoadToLDS,
    Write,
    get_custom,
)
from .compile_options import WaveCompileOptions
from .constraints import Constraint, HardwareConstraint, WaveConstraint
from .utils.general_utils import (
    get_hardware_constraint,
    get_wave_constraints,
)
from .utils.classes import GemmOperationType
from .utils.symbol_utils import get_induction_symbol

from .scheduling.scheduler_utils import (
    GemmScheduler,
)
from .scheduling.graph_utils import Edge

from .minimize_global_loads import update_write_dependencies

from .fuse_tensor_loads import _scale_distributed_shape


def add_output_to_cond(
    op: CustomOp,
    return_vals: List,
    subgraph: fx.Graph,
    parent_graph: fx.Graph,
    parent_loc: CapturedLocation,
) -> None:
    """
    Has side-effect to subgraph and parent graph.

    Args:
        op: A conditional custom op.
        return_vals: A list of return values for the conditional subgraph.
        subgraph: The conditional subgraph.
        parent_graph: The parent graph of the subgraph.
        parent_loc: The position to insert GetResult in parent graph.

    Expect Behavior:
        1. Add return to conditional subgraph.
        2. Add GetResult to parent graph.
        3. Replace all uses instead of the return op with the GetResult in step2.
    """
    assert isinstance(
        op, Conditional
    ), "Expect to add output node only on condition ops"
    if any(isinstance(get_custom(node), Output) for node in subgraph.nodes):
        return

    # add arguments to condition yield
    output_op = Output(return_vals).add_to_graph(subgraph, loc=op.location)
    output_node = get_custom(output_op)

    # replace uses of compute result by conditional returns
    with parent_graph.inserting_after(op.fx_node):
        for i, return_val in enumerate(return_vals):
            gr_node = GetResult(op.fx_node, i).add_to_graph(
                parent_graph, loc=parent_loc
            )
            gr_node.index = return_val.index
            get_custom(return_val).replace_all_uses_with_except(gr_node, [output_node])


def set_specialized_conditions(
    graph: fx.Graph,
    hardware_constraint: HardwareConstraint,
    wave_constraints: List[WaveConstraint],
) -> (fx.Node, fx.Node, IndexExpr):
    """
    Has side-effect to the graph.

    Add comparison operators for separating `load_waves` and `compute_waves` to graph.

    Expect Behavior:
        1. Compute total number of compute waves -> compute_wid (e.g., 4)
        2. Get wave IDs from linearized thread IDs.
        3. Load waves are waves with wave_id >= compute_wid (e.g., 4, 5)
        4. Compute waves are waves with wave_id < compute_wid (e.g., 0, 1, 2, 3)
        5. Add the comparison operators to the graph.
    """

    compute_wid = math.prod(map(lambda c: c.waves_per_block, wave_constraints))

    # calculate physical wid
    flat_id = hardware_constraint.linearized_thread_id
    wave_id = flat_id // hardware_constraint.threads_per_wave

    anchor = next(iter(graph.nodes))
    with graph.inserting_before(anchor):
        compute_wid_reg = NewScalar(compute_wid, tkl.i32).add_to_graph(
            graph, loc=anchor.location
        )
        wave_id_reg = NewScalar(wave_id, tkl.i32).add_to_graph(
            graph, loc=anchor.location
        )
        is_load_wave = Ge(wave_id_reg, compute_wid_reg).add_to_graph(
            graph, loc=anchor.location
        )
        is_compute_wave = Lt(wave_id_reg, compute_wid_reg).add_to_graph(
            graph, loc=anchor.location
        )

    return (is_load_wave, is_compute_wave, wave_id)


def specialize_kernel(
    trace: CapturedTrace,
    constraints: list[Constraint],
    options: WaveCompileOptions,
):
    """
    This pass has assumptions.
    * Assume SchedulingType.NONE *
    * If specialize option is set: fuse_tensor_load, add_shared_memoy_barriers passes will not run. *

    n_service_waves: int, Specified in HardwareConstraint. This is the number of service waves specified by user, waves that will only do load workloads.
    e.g., if waves_per_block = (1, 1), we launch (1, 2) waves per block, where (1, 0) is compute wave, (1, 1) is load wave.
    e.g., if waves_per_block = (2, 2), we launch (2, 3) waves per block, where wave_{0,1,2,3} are compute waves, wave_{4,5} are load waves.

    Expect Behavior:
        0. Set specialized conditions to root graph.
        1. For each iterate node, walk its subgraph, annotate ops.
        2. Partition the graph using annotations from step 1. -> Get `load_partition` and `compute_partition`
        3. Transform the graph with 2 partitions from step 2.
        4. Add epilog guard: Store results only from compute waves.
    """
    if not options.specialize:
        return

    hardware_constraint = get_hardware_constraint(constraints)
    if hardware_constraint.n_service_waves == 0:
        return

    wave_constraints = get_wave_constraints(constraints)

    is_load_wave, is_compute_wave, wave_id = set_specialized_conditions(
        trace.get_root_graph(), hardware_constraint, wave_constraints
    )

    iterate_nodes = trace.walk(lambda node: isinstance(get_custom(node), Iterate))
    if not iterate_nodes:
        return

    for iterate_node in iterate_nodes:
        iterate_op = get_custom(iterate_node)
        caller_args = list(iterate_op.implicit_captures)
        caller_args.extend([is_load_wave, is_compute_wave])
        iterate_op.implicit_captures = caller_args

        graph = trace.get_subgraph(iterate_op.subgraph_name)

        specialist = Specialist(
            trace=trace,
            graph=graph,
            edges=None,
            resources=None,
            wave_id=wave_id,
            hw=hardware_constraint,
        )

        load_partition, compute_partition = specialist.partition(iterate_op)

        # 1) Early exit if cannot find either operand's local write or global loads.
        if not load_partition or not compute_partition:
            continue

        # 2) Encapsulate the load with condition
        specialist.transform(
            iterate_op,
            is_load_wave,
            load_partition,
            is_compute_wave,
            compute_partition,
        )

    specialist.add_epilog_guard(iterate_op, is_compute_wave)
    return


class Specialist(GemmScheduler):
    """
    Specialist is responsible for converting basic gemm form:
        for i = 0 to N:
            a = READ_GLOBAL i
            WRITE_SHARED a
            b = READ_SHARED
            COMPUTE b

    into specialize form:
        for i = 0 to N:
            if wave_id == load_waves:
                a = READ_GLOBAL i
                wait empty
                WRITE_SHARED a
                signal ready
            else:
                wait ready
                b = READ_SHARED
                signal empty
                COMPUTE b
    """

    def __init__(
        self,
        trace: CapturedTrace,
        graph: fx.Graph,
        edges: list[Edge],
        resources: list[int],
        meta_name: str = "specialize",
        wave_id=None,
        hw=None,
    ) -> None:
        super().__init__(graph, edges, resources, meta_name)
        assert wave_id is not None, "Wave ID should be provided to specialist"

        self.trace: CapturedTrace = trace
        self.graph: fx.Graph = graph
        self.wave_id: IndexExpr = wave_id
        self.waves_per_block: int = [int(wpb) for wpb in hw.waves_per_block]
        self.barUB: int = math.prod(self.waves_per_block) + 1

    def add_nodes_to_graph(
        self,
        graph: fx.Graph,
        nodes: List[fx.Node],
        subs: dict[IndexSymbol, IndexSequence],
        new_writes,
        write_record,
    ):
        """
        Has side-effect to graph, new_writes and write_record.

        Args:
            graph: A graph to add nodes to.
            nodes: A list of nodes to add to the graph.
            subs: Index modifier when copying nodes to the graph.
            new_writes: The newly copied write ops, where we later need to update_write_dependencies for.
            write_record: A list of newly copied write nodes (ordered).

        Expect Behavior:
            0. Copy each node in nodes to the graph.
            1. Adjust index by the provided subs.
            2. Update new_writes and write_record if it is a Write node.
        """

        def new_index(index, shift_subs):
            return {k: v.subs(shift_subs) for k, v in index.items()}

        node_map = dict()
        for node in nodes:
            custom = get_custom(node)
            new_op = custom.copy(
                new_graph=graph,
                arg_transform=lambda x: node_map[x] if x in node_map else x,
            )
            if isinstance(custom, TensorLoadToLDS):
                # redistribute across load waves
                new_op.update_arg(
                    "distributed_shape",
                    _scale_distributed_shape(custom, self.waves_per_block[1]),
                )
                tensor_subs = {
                    THREAD_0: (THREAD_0 // 32)
                    // self.waves_per_block[0]
                    * new_op.distributed_shape[
                        get_custom(custom.dst[0]).type.symbolic_shape[0]
                    ]
                }
                tensor_subs.update(subs)
                new_op.update_arg(
                    "global_tile_index",
                    new_index(custom.global_tile_index, tensor_subs),
                )
                new_op.update_arg(
                    "shared_tile_index",
                    new_index(custom.shared_tile_index, tensor_subs),
                )
            else:
                new_op.index = new_index(node.index, subs)

            node_map[node] = new_op.fx_node

            if isinstance(custom, Write):
                new_writes[custom.memory].append(new_op.fx_node)
                write_record.append(new_op.fx_node)
            if isinstance(custom, TensorLoadToLDS):
                new_writes[custom.dst[0]].append(new_op.fx_node)
                write_record.append(new_op.fx_node)

    def get_ops_of_type(self, operation_type: GemmOperationType) -> List[fx.Node]:
        """
        Get nodes with annotation == operation_type in the graph.
        """
        return [
            node
            for node in self.graph.nodes
            if self.meta_name in node.meta
            and node.meta[self.meta_name] == operation_type
        ]

    def partition(self, iterate_op: CustomOp) -> (List[fx.Node], List[fx.Node]):
        """
        Given an iterate op, return the `load_partition` and `compute_partition`
        - Load partition includes (GLOBAL_LOAD_TO_LDS_*, GLOBAL_LOAD_*, LOCAL_WRITE_*)
        - Compute partition includes (LOCAL_LOAD_*, MMAs)
        """
        if not isinstance(iterate_op, Iterate):
            return [], []

        # 0) Get all shared write and global read.
        load_partition = []
        load_partition.extend(
            self.get_ops_of_type(GemmOperationType.GLOBAL_LOAD_TO_LDS_LHS)
        )
        load_partition.extend(
            self.get_ops_of_type(GemmOperationType.GLOBAL_LOAD_TO_LDS_RHS)
        )

        load_partition.extend(self.get_ops_of_type(GemmOperationType.LOCAL_WRITE_LHS))
        load_partition.extend(self.get_ops_of_type(GemmOperationType.LOCAL_WRITE_RHS))

        load_partition.extend(self.get_ops_of_type(GemmOperationType.GLOBAL_LOAD_LHS))
        load_partition.extend(self.get_ops_of_type(GemmOperationType.GLOBAL_LOAD_RHS))

        # Local load and mma will be handle by compute waves
        compute_partition = []

        # 1) Get all local read and wmma
        # Get MMA nodes inside a for op, who's reduction dim is being tiled in the for op.
        mma_nodes = filter_fx_graph(
            self.graph,
            lambda node: isinstance(get_custom(node), MMA)
            and get_custom(node).reduction_dim == iterate_op.axis,
        )

        compute_partition.extend(mma_nodes)
        compute_partition.extend(self.get_ops_of_type(GemmOperationType.LOCAL_LOAD_LHS))
        compute_partition.extend(self.get_ops_of_type(GemmOperationType.LOCAL_LOAD_RHS))

        return load_partition, compute_partition

    def transform(
        self,
        iterate_op: CustomOp,
        is_load_wave: fx.Node,
        load_partition: List[fx.Node],
        is_compute_wave: fx.Node,
        compute_partition: List[fx.Node],
    ) -> None:
        """
        Add a new conditional graph with `is_load_wave` as condition, load_partition as nodes.
        Add a new conditional graph with `is_copmute_wave` as condition, compute_partition as nodes.
        Add split barriers inside both conditional graphs.
        """
        # flatten all nodes
        load_flattened, _ = pytree.tree_flatten(load_partition)
        load_flattened.reverse()
        compute_flattened, _ = pytree.tree_flatten(compute_partition)
        compute_flattened.reverse()

        # Generating and inserting cond_barriers to correct place in graph.
        self.insert_load_cond(
            is_load_wave, iterate_op, load_flattened[0].location, load_flattened
        )

        self.insert_compute_cond(
            is_compute_wave,
            iterate_op,
            compute_flattened[-1].location,
            compute_flattened,
        )

        return

    def insert_compute_cond(
        self,
        cond_reg: fx.Node,
        iterate_op: CustomOp,
        location: Optional[CapturedLocation],
        nodes: List[fx.Node],
    ):
        """
        Expect Behavior:
            0. Initialize a new fx graph: compute graph.
            1. Copy the nodes to the compute graph.
            2. Add `else_return` to the condition.
            3. Add return node in the compute graph and GetResult in the parent graph.
            4. Add split barriers to the compute graph.
            5. Update trace and subgraphs in root_graph.
        """
        # declare new subgraph
        compute_graph = fx.Graph()
        compute_graph_name = f"compute_graph_{cond_reg.name}"

        # add nodes to the compute graph
        last_shared_read = None
        for node in nodes:
            op = get_custom(node)
            new_op = op.copy(new_graph=compute_graph)
            if isinstance(op, Read):
                last_shared_read = new_op.fx_node
            op.replace_all_uses_with(new_op)
            op.erase()

        # get parent graph
        pgraph = self.graph

        # add conditional nodes to parent graph
        with pgraph.inserting_before(pgraph.output_node()):
            is_compute_cond = Conditional(
                cond_reg,
                subgraph_name=compute_graph_name,
                implicit_captures=[],
                else_return=iterate_op.init_args,
            ).add_to_graph(pgraph, loc=location)

        compute_graph.parent_op = is_compute_cond

        # add return node in the compute graph and get result in parent graph
        add_output_to_cond(
            get_custom(is_compute_cond),
            iterate_op.outputs(),
            compute_graph,
            pgraph,
            location,
        )

        # add split barriers
        self.add_compute_split_barrier(compute_graph, last_shared_read)

        # update trace and root graph
        self.trace.add_subgraph(compute_graph_name, compute_graph)
        get_custom(is_compute_cond).get_root_graph().subgraphs[
            compute_graph_name
        ] = compute_graph

        return

    def insert_load_cond(
        self, cond_reg, iterate_op, location: Optional[CapturedLocation], nodes
    ):
        """
        Expect Behavior:
            0. Initialize a new fx graph: load graph.
            1. Copy the nodes to the load graph with `waves_per_block[0]` number of times.
            2. Add split barriers to the load graph.
            3. Update trace and subgraphs in root_graph.
            4. Update write dependencies with newly copied nodes in the load graph.
        """
        # load subgraph
        load_graph = fx.Graph()
        load_graph_name = f"load_graph_{cond_reg.name}"

        # add conditional at parent graph
        pgraph = self.graph

        with pgraph.inserting_before(pgraph.output_node()):
            is_load_cond = Conditional(
                cond_reg,
                subgraph_name=load_graph_name,
                implicit_captures=iterate_op.implicit_captures,
            ).add_to_graph(pgraph)

        is_load_cond.location = location
        load_graph.parent_op = is_load_cond

        # duplicate nodes
        new_writes = defaultdict(list)
        write_record = []
        for i in range(self.waves_per_block[0], 0, -1):
            shift_subs = {THREAD_1: THREAD_1 - i}

            self.add_nodes_to_graph(
                load_graph, nodes, shift_subs, new_writes, write_record
            )

        # add split barriers to load subgraph
        has_tensor_load = any(
            [isinstance(get_custom(node), TensorLoadToLDS) for node in write_record]
        )
        self.add_load_split_barrier(
            load_graph,
            iterate_op,
            write_record[0],
            write_record[-1],
            has_tdm=has_tensor_load,
        )

        # close the graph with empty output
        load_graph.output(None)

        # update trace and root graph
        self.trace.add_subgraph(load_graph_name, load_graph)
        get_custom(is_load_cond).get_root_graph().subgraphs[
            load_graph_name
        ] = load_graph

        update_write_dependencies(new_writes, self.trace)
        for node in nodes:
            node.replace_all_uses_with(None)
            get_custom(node).erase()
        return

    def add_compute_split_barrier(
        self, subgraph: fx.Graph, last_shared_read: fx.Node
    ) -> None:
        """
        Has side-effect to subgraph.

        Add split barriers to compute graph.
        Signal right after last_shared_read to unblock load waves.

        ---- Example ----
        Before:
            if is compute partition:
                LOCAL_READ
                LOCAL_READ
                MMA
        After:
            if is compute partition:
                if wave id == 0:
                    wait 1 (buffer ready)
                if wave id == 1:
                    wait 2 (buffer ready)
                ...

                LOCAL_READ
                LOCAL_READ

                if wave id == 0:
                    signal 1+4 (buffer empty)
                if wave id == 1:
                    signal 2+4 (buffer empty)

                MMA
                ...
        """
        first_node = next(iter(subgraph.nodes))
        location = get_custom(first_node).location

        with subgraph.inserting_before(first_node):
            for i in range(1, self.barUB):
                # declare graph
                wid_wait_graph = fx.Graph()
                wid_wait_graph_name = f"compute_wait_{i}_graph"

                # update trace
                self.trace.add_subgraph(wid_wait_graph_name, wid_wait_graph)

                # add wait node to wid_wait_graph
                barId = (i - 1) % self.waves_per_block[0] + 1
                SharedMemoryBarrierWait(barId=barId).add_to_graph(
                    region_graph=wid_wait_graph, loc=location
                )

                # add condition entry to parent graph (wave id is 0-based)
                cond_expr = sympy.Eq(self.wave_id, i - 1)
                wait_cond_op = Conditional(
                    cond_expr,
                    subgraph_name=wid_wait_graph_name,
                    implicit_captures=[],
                ).add_to_graph(subgraph, loc=location)

                wid_wait_graph.parent_op = wait_cond_op

                # update root graph
                get_custom(wait_cond_op).get_root_graph().subgraphs[
                    wid_wait_graph_name
                ] = wid_wait_graph

        signal_location = get_custom(last_shared_read).location
        with subgraph.inserting_after(last_shared_read):
            for i in range(1, self.barUB):
                # declare graph
                wid_signal_graph = fx.Graph()
                wid_signal_graph_name = f"compute_signal_{i}_graph"

                # update trace
                self.trace.add_subgraph(wid_signal_graph_name, wid_signal_graph)

                # add signal node to wid_signal_graph
                barId = (i - 1) % self.waves_per_block[0] + 1
                SharedMemoryBarrierSignal(barId=barId + self.barUB).add_to_graph(
                    region_graph=wid_signal_graph, loc=signal_location
                )

                # add condition entry to parent graph
                cond_expr = sympy.Eq(self.wave_id, i - 1)
                signal_cond_op = Conditional(
                    cond_expr,
                    subgraph_name=wid_signal_graph_name,
                    implicit_captures=[],
                ).add_to_graph(subgraph, loc=signal_location)

                wid_signal_graph.parent_op = signal_cond_op

                # update root graph
                get_custom(signal_cond_op).get_root_graph().subgraphs[
                    wid_signal_graph_name
                ] = wid_signal_graph

    def add_load_split_barrier(
        self,
        subgraph: fx.Graph,
        iterate_op: CustomOp,
        first_lw: fx.Node,
        last_lw: fx.Node,
        has_tdm: bool = False,
    ) -> None:
        """
        Has side-effect to subgraph.

        Add split barriers to load graph.
        Signal right after last_lw to unblock compute waves.
        Wait right before first_lw for proper synchronization.

        ---- Example ----
        Before:
            if is load partition:
                GLOBAL_READ
                GLOBAL_READ
                LOCAL_WRITE     <- first_lw
                LOCAL_WRITE     <- last_lw
        After:
            if is load partition:
                GLOBAL_READ
                GLOBAL_READ

                if not first round and wave id == 0:
                    wait 1+4 (wait buffer empty)

                if not first round and wave id == 1:
                    wait 2+4 (wait buffer empty)

                LOCAL_WRITE
                LOCAL_WRITE

                GLOBAL_READ
                GLOBAL_READ

                LOCAL_WRITE
                LOCAL_WRITE

                if wave id == 0:
                    signal 1 (signal buffer ready)

                if wave id == 1:
                    signal 2 (signal buffer ready)
                ...
        """

        # first load wave id
        start_load_wid = self.barUB - 1

        # induction symbol
        iv = get_induction_symbol(iterate_op.axis)

        with subgraph.inserting_before(first_lw):
            location = get_custom(first_lw).location
            for offset in range(1, self.waves_per_block[0] + 1):

                # i is the range of possible barrier id this load wave is helping out
                i = offset

                # declare graph
                wid_wait_graph = fx.Graph()
                wid_wait_graph_name = f"load_wait_{i}_graph"

                # update trace
                self.trace.add_subgraph(wid_wait_graph_name, wid_wait_graph)

                # add wait node to wid_wait_graph
                SharedMemoryBarrierWait(barId=i + self.barUB).add_to_graph(
                    region_graph=wid_wait_graph, loc=location
                )

                # calculate which compute wid this load wid is helping with
                compute_wid = self.wave_id % self.waves_per_block[0]
                target_id = (i - 1) % self.waves_per_block[0]

                # add condition entry to parent graph
                cond_expr = sympy.And(sympy.Eq(compute_wid, target_id), sympy.Ne(iv, 0))

                wait_cond_op = Conditional(
                    cond_expr,
                    subgraph_name=wid_wait_graph_name,
                    implicit_captures=[],
                ).add_to_graph(subgraph, loc=location)

                wid_wait_graph.parent_op = wait_cond_op

                # update root graph
                get_custom(wait_cond_op).get_root_graph().subgraphs[
                    wid_wait_graph_name
                ] = wid_wait_graph

        with subgraph.inserting_after(last_lw):
            location = get_custom(last_lw).location
            for offset in range(1, self.waves_per_block[0] + 1):

                # i is the range of possible barrier id this load wave is helping out
                i = offset

                # declare graph
                wid_signal_graph = fx.Graph()
                wid_signal_graph_name = f"load_signal_{i}_graph"

                # update trace
                self.trace.add_subgraph(wid_signal_graph_name, wid_signal_graph)

                # add signal node to wid_signal_graph
                SharedMemoryBarrierSignal(barId=i, tensor_wait=has_tdm).add_to_graph(
                    region_graph=wid_signal_graph, loc=location
                )

                # calculate which compute wid this load wid is helping with
                compute_wid = self.wave_id % self.waves_per_block[0]
                target_id = (i - 1) % self.waves_per_block[0]

                # add condition entry to parent graph
                cond_expr = sympy.Eq(compute_wid, target_id)

                signal_cond_op = Conditional(
                    cond_expr,
                    subgraph_name=wid_signal_graph_name,
                    implicit_captures=[],
                ).add_to_graph(subgraph, loc=location)

                wid_signal_graph.parent_op = signal_cond_op

                # update root graph
                get_custom(signal_cond_op).get_root_graph().subgraphs[
                    wid_signal_graph_name
                ] = wid_signal_graph

    def add_epilog_guard(self, iterate_op: CustomOp, cond_reg: fx.Node) -> None:
        """
        Add guards to the rest of the parent graph.
        Load waves does not produce results, so we store outputs only from the compute waves.
        """
        # declare new subgraph
        store_graph = fx.Graph()
        store_graph_name = f"store_graph_{cond_reg.name}"

        # add nodes to store graph
        post_iterate_node = iterate_op.graph.output_node().prev
        with store_graph.inserting_before():
            while post_iterate_node != iterate_op.fx_node:
                op = get_custom(post_iterate_node)
                new_op = op.copy(new_graph=store_graph)
                op.replace_all_uses_with(new_op)
                op.erase()
                post_iterate_node = post_iterate_node.prev

        # get parent graph
        pgraph = iterate_op.graph

        # add conditional nodes to parent graph
        with pgraph.inserting_before(pgraph.output_node()):
            is_compute_cond = Conditional(
                cond_reg,
                subgraph_name=store_graph_name,
                implicit_captures=[],
            ).add_to_graph(pgraph, loc=get_custom(pgraph.output_node()).location)

        store_graph.parent_op = is_compute_cond

        # update trace and root graph
        self.trace.add_subgraph(store_graph_name, store_graph)
        iterate_op.get_root_graph().subgraphs[store_graph_name] = store_graph
