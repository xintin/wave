# Copyright 2025 The IREE Authors
#
# Licensed under the Apache License, Version 2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
"""
Shared MLIR analysis helpers for the ASM backend.

Goal: keep MLIR scanning logic (function selection, translation_info parsing,
and workgroup-id detection) in ONE place to avoid drift across:
- (removed) `AsmEmitter.from_mlir_string`
- `driver.py`
- `KernelModuleCompiler.compile_mlir_string`
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, Tuple

from wave_lang.support.ir_imports import Operation, func_d, gpu_d, OpAttributeMap

from .utils import parse_wg_and_subgroup


def walk_ops_recursively(operation: Operation) -> Iterable[Operation]:
    """Recursively walk all operations in an MLIR operation tree."""
    for region in operation.regions:
        for block in region.blocks:
            for inner_operation in block.operations:
                yield inner_operation
                yield from walk_ops_recursively(inner_operation)


# ============================================================================
# Kernel Selection Policy
# ============================================================================
# These patterns define which MLIR functions are skipped during compilation.
# The Wave compiler generates wrapper functions that should not be compiled
# as GPU kernels:
#
# 1. "isolated_benchmark*" - Host-side benchmark scaffolding generated by IREE
# 2. "*$async" - Async variant wrappers (the actual kernel is the non-$async one)
#
# If you encounter "named symbol not found" or "no kernel image" errors,
# verify this policy is correct for your MLIR naming conventions.
# ============================================================================

# Patterns for functions that should NOT be treated as GPU kernels
_SKIP_FUNCTION_PREFIXES = ("isolated_benchmark",)
_SKIP_FUNCTION_SUFFIXES = ("$async",)


def should_skip_function(fn: func_d.FuncOp) -> bool:
    """
    Return True if this function should not be treated as a kernel.

    This function implements the kernel selection policy for the ASM backend.
    Functions are skipped if they match any of the patterns defined in
    _SKIP_FUNCTION_PREFIXES or _SKIP_FUNCTION_SUFFIXES.

    Args:
        fn: The MLIR function operation to check

    Returns:
        True if the function should be skipped (not compiled as a kernel)

    Note:
        If this returns True for your actual kernel function, you will get
        "named symbol not found" or "no kernel image" runtime errors.
    """
    name = fn.sym_name.value

    for prefix in _SKIP_FUNCTION_PREFIXES:
        if name.startswith(prefix):
            return True

    for suffix in _SKIP_FUNCTION_SUFFIXES:
        if name.endswith(suffix):
            return True

    return False


def detect_needed_workgroup_ids(fn: func_d.FuncOp) -> Tuple[bool, bool, bool]:
    """
    Scan MLIR function to detect which workgroup IDs are needed.

    Returns:
        (needs_wgid_x, needs_wgid_y, needs_wgid_z)
    """
    needs_x, needs_y, needs_z = False, False, False

    def walk_ops(op):
        nonlocal needs_x, needs_y, needs_z

        if isinstance(op, gpu_d.BlockIdOp):
            dim_str = str(op.dimension)
            if "dim x" in dim_str:
                needs_x = True
            elif "dim y" in dim_str:
                needs_y = True
            elif "dim z" in dim_str:
                needs_z = True

        if hasattr(op, "regions"):
            for region in op.regions:
                for block in region.blocks:
                    for inner_op in block.operations:
                        walk_ops(inner_op)

    walk_ops(fn)
    return (needs_x, needs_y, needs_z)


@dataclass(frozen=True)
class TranslationInfo:
    wg_size: Tuple[int, int, int]
    subgroup_size: int


def extract_translation_info(
    fn: func_d.FuncOp, require_translation_info: bool = True
) -> TranslationInfo:
    """Extract (wg_size, subgroup_size) from translation_info attributes.

    Args:
        fn: The MLIR function operation
        require_translation_info: If True, raise an error when translation_info
            is missing. If False, use defaults (64, 1, 1) and 64.

    Raises:
        ValueError: If require_translation_info=True and the attribute is missing.
    """
    function_attributes = (
        dict(fn.attributes) if isinstance(fn.attributes, OpAttributeMap) else {}
    )
    translation_info = function_attributes.get("translation_info")

    if translation_info is None:
        if require_translation_info:
            fn_name = fn.name.value if hasattr(fn.name, "value") else str(fn.name)
            raise ValueError(
                f"Function '{fn_name}' is missing the required 'translation_info' "
                f"attribute. This attribute specifies workgroup_size and subgroup_size. "
                f"Ensure the MLIR has been properly lowered with IREE's codegen pipeline."
            )
        # Fallback defaults for test cases or special scenarios
        return TranslationInfo(wg_size=(64, 1, 1), subgroup_size=64)

    # Parse translation_info
    workgroup_size_tuple, sg_size = parse_wg_and_subgroup(translation_info)

    # Normalize workgroup size to 3-tuple
    wg_size: Tuple[int, int, int] = (64, 1, 1)
    if workgroup_size_tuple:
        if len(workgroup_size_tuple) == 3:
            wg_size = workgroup_size_tuple
        elif len(workgroup_size_tuple) == 2:
            wg_size = (workgroup_size_tuple[0], workgroup_size_tuple[1], 1)
        elif len(workgroup_size_tuple) == 1:
            wg_size = (workgroup_size_tuple[0], 1, 1)

    subgroup_size = sg_size if sg_size else 64
    if not sg_size:
        import warnings

        fn_name = fn.name.value if hasattr(fn.name, "value") else str(fn.name)
        warnings.warn(
            f"Function '{fn_name}' has translation_info but no subgroup_size. "
            f"Defaulting to 64.",
            UserWarning,
        )

    return TranslationInfo(wg_size=wg_size, subgroup_size=subgroup_size)
